{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.fit() fine tune the RNN hidden layer parameter\n",
    "#### layers = [200, 250, 300,'200-100','300-200','200-100-50', '300-200-100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 15:54:07.833364: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-08 15:54:08.287182: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1895410000 Hz\n",
      "2019-12-08 15:54:08.287974: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4aa8730 executing computations on platform Host. Devices:\n",
      "2019-12-08 15:54:08.288015: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "Train on 177316 samples, validate on 629 samples\n",
      "Epoch 1/10\n",
      "2019-12-08 15:54:21.409023: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_5815_7274' and '__inference___backward_standard_lstm_7392_7991_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8128' both implement 'lstm_7785e9da-2736-4568-83d9-ae5761a6bcfe' but their signatures do not match.\n",
      "2019-12-08 15:58:19.670679: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_19446' and '__inference_standard_lstm_19446_specialized_for_sequential_lstm_StatefulPartitionedCall_at___inference_distributed_function_21314' both implement 'lstm_170e8dd6-706a-45c7-be9e-bfbdd1728e8e' but their signatures do not match.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.89269, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.72_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 243s - loss: 6.9481 - val_loss: 6.8927\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.89269 to 6.84715, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.72_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 248s - loss: 6.8792 - val_loss: 6.8472\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.84715 to 6.80179, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.72_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 242s - loss: 6.8142 - val_loss: 6.8018\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.80179 to 6.75154, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.72_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 246s - loss: 6.7803 - val_loss: 6.7515\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.75154\n",
      "177316/177316 - 246s - loss: 6.7336 - val_loss: 6.7557\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.75154 to 6.71344, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.72_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 244s - loss: 6.6774 - val_loss: 6.7134\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.71344 to 6.65282, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.72_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 241s - loss: 6.6374 - val_loss: 6.6528\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: val_loss improved from 6.65282 to 6.61956, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.72_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 242s - loss: 6.6015 - val_loss: 6.6196\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: val_loss improved from 6.61956 to 6.59987, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.72_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 241s - loss: 6.5606 - val_loss: 6.5999\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.59987\n",
      "177316/177316 - 241s - loss: 6.5231 - val_loss: 6.6041\n",
      "[6.94811860379506, 6.879202375879046, 6.814226186365032, 6.78034038051471, 6.73357450857457, 6.677416436217038, 6.637433948715631, 6.6015215811458035, 6.560601436996899, 6.523124260616167]\n",
      "Opening file (1)\n",
      "RNN with categorical cross entropy 0 batchs,  2434.3624789714813  epochs in 2487.863559484482 s\n",
      "Last train cost :  6.7155559718819955\n",
      "recall :  0.04926736139584846\n",
      "precision :  0.08028616852146298\n",
      "sps :  0.05087440381558029\n",
      "Best  sps :  0.05087440381558029\n",
      "user_coverage :  0.534181240063593\n",
      "item_coverage :  115\n",
      "ndcg :  0.08527619516459846\n",
      "blockbuster_share :  0.45544554455445546\n",
      "-----------------\n",
      "Save model in ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2434.362_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n"
     ]
    }
   ],
   "source": [
    "!python train.py -d ks-cooks-1y -b 32 --max_length 60 --r_emb 100 --min_iter 10 --r_l 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 16:35:51.933258: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-08 16:35:51.955102: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1895410000 Hz\n",
      "2019-12-08 16:35:51.955487: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x45ffb60 executing computations on platform Host. Devices:\n",
      "2019-12-08 16:35:51.955512: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "Train on 177316 samples, validate on 629 samples\n",
      "Epoch 1/10\n",
      "2019-12-08 16:35:59.721389: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_5815_7274' and '__inference___backward_standard_lstm_7392_7991_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8128' both implement 'lstm_d1d22693-0e96-422a-80aa-cd579da21dbc' but their signatures do not match.\n",
      "2019-12-08 16:39:57.143985: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_19446' and '__inference_standard_lstm_19446_specialized_for_sequential_lstm_StatefulPartitionedCall_at___inference_distributed_function_21314' both implement 'lstm_163e9693-cbdd-42b9-8109-17aff02187e6' but their signatures do not match.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.89269, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.708_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 241s - loss: 6.9481 - val_loss: 6.8927\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.89269 to 6.84715, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.708_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 246s - loss: 6.8792 - val_loss: 6.8472\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.84715 to 6.80179, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.708_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 248s - loss: 6.8142 - val_loss: 6.8018\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.80179 to 6.75154, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.708_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 248s - loss: 6.7803 - val_loss: 6.7515\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.75154\n",
      "177316/177316 - 247s - loss: 6.7336 - val_loss: 6.7557\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.75154 to 6.71344, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.708_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 248s - loss: 6.6774 - val_loss: 6.7134\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.71344 to 6.65282, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.708_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 250s - loss: 6.6374 - val_loss: 6.6528\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: val_loss improved from 6.65282 to 6.61956, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.708_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 249s - loss: 6.6015 - val_loss: 6.6196\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: val_loss improved from 6.61956 to 6.59987, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.708_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 252s - loss: 6.5606 - val_loss: 6.5999\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.59987\n",
      "177316/177316 - 248s - loss: 6.5231 - val_loss: 6.6041\n",
      "[6.94811860379506, 6.879202375879046, 6.814226186365032, 6.78034038051471, 6.73357450857457, 6.677416436217038, 6.637433948715631, 6.6015215811458035, 6.560601436996899, 6.523124260616167]\n",
      "Opening file (1)\n",
      "RNN with categorical cross entropy 0 batchs,  2478.666837453842  epochs in 2530.4522364139557 s\n",
      "Last train cost :  6.7155559718819955\n",
      "recall :  0.04926736139584846\n",
      "precision :  0.08028616852146298\n",
      "sps :  0.05087440381558029\n",
      "Best  sps :  0.05087440381558029\n",
      "user_coverage :  0.534181240063593\n",
      "item_coverage :  115\n",
      "ndcg :  0.08527619516459846\n",
      "blockbuster_share :  0.45544554455445546\n",
      "-----------------\n",
      "Save model in ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2478.667_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n"
     ]
    }
   ],
   "source": [
    "!python train.py -d ks-cooks-1y -b 32 --max_length 60 --r_emb 100 --min_iter 10 --r_l 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 17:34:08.532884: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-08 17:34:08.555037: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1895410000 Hz\n",
      "2019-12-08 17:34:08.555368: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3a404b0 executing computations on platform Host. Devices:\n",
      "2019-12-08 17:34:08.555398: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "Train on 177316 samples, validate on 629 samples\n",
      "Epoch 1/10\n",
      "2019-12-08 17:34:16.831924: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_7372_7971_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8090' and '__inference___backward_standard_lstm_7372_7971' both implement 'lstm_ca7d74f5-8966-4d30-a3d0-8c91dc5518f2' but their signatures do not match.\n",
      "2019-12-08 17:38:08.120581: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_19753' and '__inference_standard_lstm_19410_specialized_for_sequential_lstm_StatefulPartitionedCall_at___inference_distributed_function_21278' both implement 'lstm_6b1aa37b-2106-4db4-95ff-f9209658ce8f' but their signatures do not match.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.80510, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.917_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 235s - loss: 6.9004 - val_loss: 6.8051\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.80510 to 6.74858, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.917_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 242s - loss: 6.7871 - val_loss: 6.7486\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.74858 to 6.72847, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.917_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 246s - loss: 6.7379 - val_loss: 6.7285\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.72847 to 6.69804, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.917_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 244s - loss: 6.6957 - val_loss: 6.6980\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss improved from 6.69804 to 6.68258, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.917_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 242s - loss: 6.6618 - val_loss: 6.6826\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.68258 to 6.65406, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.917_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 242s - loss: 6.6316 - val_loss: 6.6541\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.65406 to 6.63996, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.917_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 243s - loss: 6.6050 - val_loss: 6.6400\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: val_loss improved from 6.63996 to 6.60451, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.917_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 243s - loss: 6.5797 - val_loss: 6.6045\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.60451\n",
      "177316/177316 - 244s - loss: 6.5541 - val_loss: 6.6425\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.60451\n",
      "177316/177316 - 242s - loss: 6.5299 - val_loss: 6.6855\n",
      "[6.900357414075562, 6.787100696174325, 6.737906932411296, 6.695735833090302, 6.661755012092779, 6.631556141537578, 6.605017030754624, 6.579655930363927, 6.554129551151297, 6.529854615081179]\n",
      "Opening file (1)\n",
      "RNN with categorical cross entropy 0 batchs,  2422.526834487915  epochs in 2476.2656564712524 s\n",
      "Last train cost :  6.668306915673287\n",
      "recall :  0.04841186986597278\n",
      "precision :  0.07758346581876029\n",
      "sps :  0.058823529411764705\n",
      "Best  sps :  0.058823529411764705\n",
      "user_coverage :  0.505564387917329\n",
      "item_coverage :  114\n",
      "ndcg :  0.08390476185460839\n",
      "blockbuster_share :  0.45081967213114754\n",
      "-----------------\n",
      "Save model in ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2422.527_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n"
     ]
    }
   ],
   "source": [
    "!python train.py -d ks-cooks-1y -b 32 --max_length 60 --r_l 100  --min_iter 10 --r_emb 100 --r_emb_opt lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-08 18:27:50.688075: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-08 18:27:50.711084: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1895410000 Hz\n",
      "2019-12-08 18:27:50.711400: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3a21750 executing computations on platform Host. Devices:\n",
      "2019-12-08 18:27:50.711426: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "Train on 177316 samples, validate on 629 samples\n",
      "Epoch 1/10\n",
      "2019-12-08 18:27:58.865701: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_7372_7971_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8090' and '__inference___backward_standard_lstm_7372_7971' both implement 'lstm_de970266-ff0e-44bc-aae5-3c26c3036a8c' but their signatures do not match.\n",
      "2019-12-08 18:31:49.505941: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_19753' and '__inference_standard_lstm_19410_specialized_for_sequential_lstm_StatefulPartitionedCall_at___inference_distributed_function_21278' both implement 'lstm_929bffde-54a9-4807-a465-d8605a0ef50c' but their signatures do not match.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.85168, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.825_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 234s - loss: 6.9328 - val_loss: 6.8517\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.85168 to 6.76717, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.825_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 240s - loss: 6.8109 - val_loss: 6.7672\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.76717 to 6.76570, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.825_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 241s - loss: 6.7748 - val_loss: 6.7657\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.76570 to 6.73645, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.825_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 242s - loss: 6.7466 - val_loss: 6.7364\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.73645\n",
      "177316/177316 - 237s - loss: 6.7159 - val_loss: 6.7431\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.73645 to 6.73055, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.825_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 243s - loss: 6.6953 - val_loss: 6.7306\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.73055 to 6.70131, saving model to ks-cooks-1y/models/rnn_cce_ml60_bs32_ne2.825_gc100_e100_h100_Ug_lr0.1_nt1.hdf5\n",
      "177316/177316 - 243s - loss: 6.6756 - val_loss: 6.7013\n",
      "Epoch 8/10\n"
     ]
    }
   ],
   "source": [
    "!python train.py -d ks-cooks-1y -b 32 --max_length 60 --r_l 100  --min_iter 10 --r_emb 100 --r_emb_opt tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py -d ks-cooks-1y -b 32 --max_length 60 --r_l 100 --min_iter 10 --r_emb 300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py -d ks-cooks-1y -b 32 --max_length 60 --r_l 100 --min_iter 10 --r_emb 300 --r_emb_opt lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py -d ks-cooks-1y -b 32 --max_length 60 --r_l 100 --min_iter 10 --r_emb 300 --r_emb_opt tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull origin remote_\n",
    "!git add .\n",
    "!git commit -m 'Comparing three embedding methods'\n",
    "!git push -u origin master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_remote_env_py3",
   "language": "python",
   "name": "my_remote_env_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
